{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the note of the first Week\n",
    "### Also check the slide I catch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias and Variance\n",
    "\n",
    "- bias 偏差 (high error)\n",
    "- Variance 方差 （overfitting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic recipe for ML \n",
    "\n",
    "- high bias?  ----> yes ---> \n",
    "- - bigger NN + Trianing longer+( other structure) \n",
    "- high variance?\n",
    "\n",
    "\"Bias variance tradeoff\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulazation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try the basic dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[2,3,4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dimension of the a matrix is 2&3\n",
      "[[False  True False]\n",
      " [ True False  True]]\n",
      "[[0 2 0]\n",
      " [2 0 4]]\n"
     ]
    }
   ],
   "source": [
    "x= a.shape[0]\n",
    "y= a.shape[1]\n",
    "keep_prob = 0.8\n",
    "d3 =np.random.rand(a.shape[0],a.shape[1])<keep_prob #here is random determined the desliminate part exist broadcasting\n",
    "a3 = np. multiply(a,d3)\n",
    "print(\"the dimension of the a matrix is \" +str(x)+\"&\"+str(y))\n",
    "print(d3)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep the expectation value of the a3 stay the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  2.5 0. ]\n",
      " [2.5 0.  5. ]]\n"
     ]
    }
   ],
   "source": [
    "c3 = a3/keep_prob#\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar to L2 norm\n",
    "So if worried about sone layer will overfitting, you can increas the key prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pros and cons\n",
    "- advantage: A good regulazation way, useful when you have small data sets: *the computer vision* as a example\n",
    "- downside: no well defined *cost function J* So you are losing the way which plots J to debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to  solve this problem \n",
    "- first, turn off the *drop out*\n",
    "- doing the nomal debugging *whether the J is decreasing for every iteration*\n",
    "- finish it, than open the *drop out* to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods to do the regulazation\n",
    "#### Date augmentation ---> fake trainning example\n",
    "- flipping the images to double the data sets \n",
    "- random transation \n",
    "- - can avoid the overfitting\n",
    "- ditorition adapt\n",
    "\n",
    "#### Early Stopping --->stop ensure the mid-size omega\n",
    " \n",
    "\n",
    "- plot the the trainning error (or the J)\n",
    "- + plot the dev set error\n",
    "- #advantage: using this method you can just doing one interation rather than find the hyperparameter(computationally expensive)\n",
    "\n",
    "#### L2 Regulazation\n",
    "\n",
    "\n",
    "*some conception called the orthognalization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$x^2 \\quad\n",
       "\\text{this is try the function of the Jupyter}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex \n",
    "$x^2 \\quad\n",
    "\\text{this is try the function of the Jupyter}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your optimize problem\n",
    "## Normalizing the trainning sets\n",
    "- basic idea is that:\n",
    "\n",
    "$$ \\bar{x} = \\frac{x-\\mu}{\\sigma}$$\n",
    "\n",
    "- so advantage is the gradinet will be much fast\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanishin / exploding gradients\n",
    "\n",
    "assume that we had $\\omega^{[l]}$\n",
    "\n",
    "$$ \\omega^{[l]} = \\left[\\begin{array}{l}\n",
    "a & 0\\\\\n",
    "0 & b\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "if we had \n",
    "\n",
    "$$ \\omega^{[l]} > I \\text{ the gradients will be exploding}$$\n",
    "\n",
    "$$ \\omega^{[l]} < I \\text{ the gradients will be vanishing}$$\n",
    "\n",
    "they will exist will be clear when the network going deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization for Deep Networks\n",
    "\n",
    "- try to set the $ \\omega$  to the 1 which help vanishing or exploding\n",
    "- - and you check the detail in lecture\n",
    "\n",
    "- single neuron example \n",
    "\n",
    "#### and some example for \n",
    "\n",
    "$$var(\\omega:)$$\n",
    "\n",
    "- for tanh ---> xavies innitialzation:  $\\sqrt{\\frac{1}{n^{[l-1]}}}$\n",
    "\n",
    "- for Relu --->  $\\sqrt{\\frac{2}{n^{[l-1]}}}$\n",
    " \n",
    "- some others --->   $\\sqrt{\\frac{2}{n^{[l-1]}+n^{[l]}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refine the backpropagation:\n",
    "#### Numerical calculate the gradient(in Machine learning couse skip)\n",
    "\n",
    "#### Gradient checking( skip)\n",
    "\n",
    "- reshape into a giant vector\n",
    "-  $10^{-7}$  therohold\n",
    " \n",
    "#### some notes \n",
    "\n",
    "- dont use in training(slow) --only debug\n",
    "- identify the bug\n",
    "- remeber regularization\n",
    "- of course doesn't work with dropout( back to the dropout part)\n",
    "- run the gradient check can be both in the:\n",
    "- - initial part\n",
    "- - some step after interation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <h4 style = 'color: orange'> the end of the week1 (temporate) </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So here is the note for week6 Machine Learning\n",
    "##  Model selection problem\n",
    "#### the data was seperated in \n",
    "- training data set\n",
    "- cross valiadation set\n",
    "- test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same problem for the bias and the variance *with different method to deal it*\n",
    "- high bias(underfit)\n",
    "- high variance (overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper-parameter Tuning, Optimization\n",
    "\n",
    "## Classification\n",
    "\n",
    "### layers\n",
    "\n",
    "### hidden units\n",
    "\n",
    "### activation function\n",
    "\n",
    "### learning rate\n",
    "\n",
    "## data sets \n",
    "\n",
    "### Train\n",
    "\n",
    "### Dev\n",
    "\n",
    "- Usiing this part to select the algorithom\n",
    "\n",
    "### test\n",
    "\n",
    "## The Problem may cause\n",
    "\n",
    "### Bias\n",
    "\n",
    "- high\n",
    "\n",
    "\t- underfitted\n",
    "\n",
    "- low\n",
    "\n",
    "\t- overfitted\n",
    "\n",
    "### Variance\n",
    "\n",
    "- high \n",
    "\n",
    "\t- overfitted\n",
    "\n",
    "- low\n",
    "\n",
    "\t- underfitted\n",
    "\n",
    "### Problem Solving\n",
    "\n",
    "- change the hyper-parameters\n",
    "\n",
    "\t- logistic Regression\n",
    "\n",
    "\t\t- L_1 + L_2 regularization\n",
    "\t\t- Dropout Regularization\n",
    "\n",
    "\t\t\t- Inverted Drop out:on the notion\n",
    "\n",
    "\t\t\t\t- On (training process)\n",
    "\t\t\t\t- Off( test process)\n",
    "\n",
    "\t\t- Early Stopping(on the slide1)\n",
    "\n",
    "\t\t\t- Watch the curves\n",
    "\n",
    "- Optimize problem\n",
    "\n",
    "\t- Vanishing/ exploding Gradients)\n",
    "\n",
    "\t\t- Weight initialization \n",
    "\n",
    "\t\t\t- let w ---> 1 \n",
    "\n",
    "\t\t\t\t- for tanh (xavis)\n",
    "\t\t\t\t- Relu\n",
    "\t\t\t\t- others\n",
    "\n",
    "### Back Propagation accuracy checking \n",
    "\n",
    "- Gradient Checking\n",
    "\n",
    "## Optimization MEthod\n",
    "\n",
    "### Mini Batch Method\n",
    "\n",
    "- chose the size\n",
    "- ramdom position implement\n",
    "- Gradient decent\n",
    "\n",
    "### Exponentially Weighted average\n",
    "\n",
    "- Bias correction\n",
    "\n",
    "## Hyper-parameter choose\n",
    "\n",
    "### Don't using grid\n",
    "\n",
    "- using random values\n",
    "\n",
    "### Coarse to Fine\n",
    "\n",
    "### Properly scaling\n",
    "\n",
    "### Two model to find Hyper\n",
    "\n",
    "- panda\n",
    "- cavia\n",
    "\n",
    "### Normalizing speed up\n",
    "\n",
    "- batch Norm\n",
    "\n",
    "\t- has slight Regularization effect\n",
    "\n",
    "## multi-class Recognition\n",
    "\n",
    "### SoftMax method\n",
    "\n",
    "*XMind - Trial Version*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
